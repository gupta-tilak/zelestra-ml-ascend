{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64b88b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Solar Panel Prediction Pipeline...\n",
      "Error in prediction pipeline: Model file not found: \n",
      "Pipeline failed: Model file not found: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vy/hqlqcg292rj1msf91q8wjp_40000gn/T/ipykernel_52355/741362664.py\", line 158, in load_trained_model\n",
      "    with open(self.model_path, 'rb') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guptatilak/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 326, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: ''\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/vy/hqlqcg292rj1msf91q8wjp_40000gn/T/ipykernel_52355/741362664.py\", line 482, in main\n",
      "    pipeline = SolarPanelPredictionPipeline(model_path='')\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/vy/hqlqcg292rj1msf91q8wjp_40000gn/T/ipykernel_52355/741362664.py\", line 151, in __init__\n",
      "    self.load_trained_model()\n",
      "  File \"/var/folders/vy/hqlqcg292rj1msf91q8wjp_40000gn/T/ipykernel_52355/741362664.py\", line 182, in load_trained_model\n",
      "    raise FileNotFoundError(f\"Model file not found: {self.model_path}\")\n",
      "FileNotFoundError: Model file not found: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# TensorFlow/Keras imports for the ANNRegressor\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.regularizers import l1_l2\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: TensorFlow not available. ANNRegressor will not work.\")\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Include the exact ANNRegressor class from training\n",
    "class ANNRegressor:\n",
    "    \"\"\"\n",
    "    Custom ANN Regressor wrapper that mimics scikit-learn interface\n",
    "    This is the exact class definition used during model training.\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons=128, layers=3, dropout_rate=0.3, \n",
    "                 learning_rate=0.001, l1_reg=0.0, l2_reg=0.01,\n",
    "                 epochs=200, batch_size=32, validation_split=0.2,\n",
    "                 patience=20, verbose=0):\n",
    "        self.neurons = neurons\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "        self.history_ = None\n",
    "        \n",
    "    def _build_model(self, input_dim):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        if not TENSORFLOW_AVAILABLE:\n",
    "            raise ImportError(\"TensorFlow is required for ANNRegressor but is not installed.\")\n",
    "            \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(self.neurons, \n",
    "                       input_dim=input_dim,\n",
    "                       activation='relu',\n",
    "                       kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.layers - 1):\n",
    "            layer_neurons = max(self.neurons // (2 ** i), 32)\n",
    "            model.add(Dense(layer_neurons,\n",
    "                           activation='relu',\n",
    "                           kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the neural network\"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        if len(y.shape) > 1:\n",
    "            y = y.flatten()\n",
    "        \n",
    "        self.model_ = self._build_model(X.shape[1])\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=self.patience, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(patience=self.patience//2, factor=0.5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        self.history_ = self.model_.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        predictions = self.model_.predict(X, verbose=0)\n",
    "        return predictions.flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'neurons': self.neurons,\n",
    "            'layers': self.layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'l1_reg': self.l1_reg,\n",
    "            'l2_reg': self.l2_reg,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'validation_split': self.validation_split,\n",
    "            'patience': self.patience,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "class SolarPanelPredictionPipeline:\n",
    "    def __init__(self, model_path='model/best_solar_model.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize prediction pipeline with trained model\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model_package = None\n",
    "        self.load_trained_model()\n",
    "    \n",
    "    def load_trained_model(self):\n",
    "        \"\"\"\n",
    "        Load the trained model and all preprocessing components\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                self.model_package = pickle.load(f)\n",
    "            \n",
    "            # Load all components from the training pipeline\n",
    "            self.best_model = self.model_package['model']\n",
    "            self.preprocessor = self.model_package['preprocessor']\n",
    "            self.target_transformer = self.model_package['target_transformer']\n",
    "            self.imputer = self.model_package.get('imputer', None)\n",
    "            self.feature_cols = self.model_package['feature_names']\n",
    "            self.categorical_cols = self.model_package['categorical_cols']\n",
    "            self.numerical_cols = self.model_package['numerical_cols']\n",
    "            self.best_model_name = self.model_package.get('best_model_name', 'Unknown')\n",
    "            self.best_score = self.model_package.get('best_score', 0)\n",
    "            self.features_to_drop = self.model_package.get('features_to_drop', [])\n",
    "            self.final_feature_names = self.model_package.get('final_feature_names', [])\n",
    "            \n",
    "            print(f\"Model loaded successfully: {self.best_model_name}\")\n",
    "            print(f\"Model score: {self.best_score:.4f}\")\n",
    "            print(f\"Expected features: {len(self.feature_cols)}\")\n",
    "            print(f\"Features to drop: {self.features_to_drop}\")\n",
    "            print(f\"Categorical features: {len(self.categorical_cols)}\")\n",
    "            print(f\"Numerical features: {len(self.numerical_cols)}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.model_path}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading model: {str(e)}\")\n",
    "    \n",
    "    def fix_data_types(self, df, dataset_name=\"PREDICTION\"):\n",
    "        \"\"\"\n",
    "        Fix data type inconsistencies for specific columns\n",
    "        This matches the exact logic from the training pipeline\n",
    "        \"\"\"\n",
    "        df_fixed = df.copy()\n",
    "        \n",
    "        # Define columns that should be numeric (same as training)\n",
    "        numeric_columns_to_fix = ['humidity', 'wind_speed', 'pressure']\n",
    "        \n",
    "        print(f\"\\n=== FIXING DATA TYPES FOR {dataset_name} ===\")\n",
    "        \n",
    "        for col in numeric_columns_to_fix:\n",
    "            if col in df_fixed.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                print(f\"Original dtype: {df_fixed[col].dtype}\")\n",
    "                \n",
    "                if df_fixed[col].dtype == 'object':\n",
    "                    try:\n",
    "                        numeric_conversion = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        non_numeric_mask = pd.isna(numeric_conversion) & df_fixed[col].notna()\n",
    "                        \n",
    "                        if non_numeric_mask.any():\n",
    "                            print(f\"Non-numeric values found in {col}:\")\n",
    "                            non_numeric_values = df_fixed.loc[non_numeric_mask, col].value_counts()\n",
    "                            print(non_numeric_values.head(10))\n",
    "                            \n",
    "                            df_fixed[col] = df_fixed[col].astype(str)\n",
    "                            df_fixed[col] = df_fixed[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "                            df_fixed[col] = df_fixed[col].str.strip()\n",
    "                            df_fixed[col] = df_fixed[col].replace('', np.nan)\n",
    "                            df_fixed[col] = df_fixed[col].replace('nan', np.nan)\n",
    "                            \n",
    "                        df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        \n",
    "                        print(f\"Converted dtype: {df_fixed[col].dtype}\")\n",
    "                        print(f\"Missing values after conversion: {df_fixed[col].isnull().sum()}\")\n",
    "                        print(f\"Valid numeric values: {df_fixed[col].notna().sum()}\")\n",
    "                        \n",
    "                        if df_fixed[col].notna().any():\n",
    "                            print(f\"Min: {df_fixed[col].min():.3f}\")\n",
    "                            print(f\"Max: {df_fixed[col].max():.3f}\")\n",
    "                            print(f\"Mean: {df_fixed[col].mean():.3f}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting {col}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"{col} is already numeric type: {df_fixed[col].dtype}\")\n",
    "        \n",
    "        return df_fixed\n",
    "    \n",
    "    def apply_imputation(self, df):\n",
    "        \"\"\"\n",
    "        Apply the fitted imputation pipeline to the raw data\n",
    "        \"\"\"\n",
    "        if self.imputer is None:\n",
    "            print(\"Warning: No imputation pipeline found. Proceeding without imputation.\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"\\n=== APPLYING IMPUTATION PIPELINE ===\")\n",
    "        print(f\"Data shape before imputation: {df.shape}\")\n",
    "        print(f\"Missing values before imputation:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
    "        \n",
    "        # df_imputed = self.imputer.transform(df)\n",
    "\n",
    "        df_imputed= self.imputer.transform_prediction(df)\n",
    "        \n",
    "        print(f\"Data shape after imputation: {df_imputed.shape}\")\n",
    "        remaining_missing = df_imputed.isnull().sum().sum()\n",
    "        print(f\"Remaining missing values after imputation: {remaining_missing}\")\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def apply_feature_engineering(self, df):\n",
    "        \"\"\"\n",
    "        Apply the same feature engineering as used during training\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== APPLYING FEATURE ENGINEERING ===\")\n",
    "        print(f\"Data shape before feature engineering: {df.shape}\")\n",
    "        \n",
    "        try:\n",
    "            from utils.feature_engineering import SolarFeatureEngineering\n",
    "            feature_engineer = SolarFeatureEngineering()\n",
    "            df_engineered = feature_engineer.create_solar_features(df)\n",
    "            \n",
    "            print(f\"Data shape after feature engineering: {df_engineered.shape}\")\n",
    "            \n",
    "            # Verify no missing values in new features\n",
    "            new_missing = df_engineered.isnull().sum().sum()\n",
    "            if new_missing > 0:\n",
    "                print(f\"Warning: {new_missing} missing values found after feature engineering\")\n",
    "                print(\"Missing values by column:\")\n",
    "                print(df_engineered.isnull().sum()[df_engineered.isnull().sum() > 0])\n",
    "            \n",
    "            return df_engineered\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Warning: feature_engineering module not found. Skipping feature engineering.\")\n",
    "            print(\"This may cause prediction errors if the model expects engineered features.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature engineering: {str(e)}\")\n",
    "            print(\"Proceeding without feature engineering.\")\n",
    "            return df\n",
    "    \n",
    "    def drop_features(self, df):\n",
    "        \"\"\"\n",
    "        Drop the same features that were dropped during training\n",
    "        \"\"\"\n",
    "        if not self.features_to_drop:\n",
    "            print(\"No features to drop.\")\n",
    "            return df\n",
    "            \n",
    "        print(f\"\\n=== DROPPING FEATURES ===\")\n",
    "        print(f\"Features to drop: {self.features_to_drop}\")\n",
    "        \n",
    "        available_features = [col for col in self.features_to_drop if col in df.columns]\n",
    "        unavailable_features = [col for col in self.features_to_drop if col not in df.columns]\n",
    "        \n",
    "        if available_features:\n",
    "            df_dropped = df.drop(columns=available_features)\n",
    "            print(f\"Dropped features: {available_features}\")\n",
    "            print(f\"Data shape after dropping features: {df_dropped.shape}\")\n",
    "        else:\n",
    "            df_dropped = df\n",
    "            print(\"No features were dropped (none found in dataset)\")\n",
    "        \n",
    "        if unavailable_features:\n",
    "            print(f\"Warning: Features not found in dataset: {unavailable_features}\")\n",
    "        \n",
    "        return df_dropped\n",
    "    \n",
    "    def preprocess_features(self, df):\n",
    "        \"\"\"\n",
    "        Apply the same preprocessing as used during training\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== PREPROCESSING FEATURES ===\")\n",
    "        \n",
    "        # Step 1: Select only the features that were used during training\n",
    "        missing_features = [col for col in self.feature_cols if col not in df.columns]\n",
    "        if missing_features:\n",
    "            print(f\"Error: Missing required features: {missing_features}\")\n",
    "            raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "        \n",
    "        X = df[self.feature_cols].copy()\n",
    "        print(f\"Selected feature columns: {X.shape[1]} features\")\n",
    "        \n",
    "        # Step 2: Apply preprocessing using fitted preprocessor\n",
    "        X_processed = self.preprocessor.transform(X)\n",
    "        \n",
    "        # Step 3: Convert back to DataFrame using the same feature names as training\n",
    "        X_processed = pd.DataFrame(X_processed, columns=self.final_feature_names, index=X.index)\n",
    "        \n",
    "        print(f\"✓ Applied preprocessing\")\n",
    "        print(f\"Final preprocessed data shape: {X_processed.shape}\")\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_pred):\n",
    "        \"\"\"\n",
    "        Apply inverse transformation to predictions to get them back to original scale\n",
    "        \"\"\"\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_pred_original = self.target_transformer.inverse_transform(y_pred_reshaped).flatten()\n",
    "        return y_pred_original\n",
    "    \n",
    "    def predict(self, raw_data):\n",
    "        \"\"\"\n",
    "        Complete prediction pipeline that mirrors the training pipeline exactly\n",
    "        \n",
    "        Pipeline: Raw Data → Fix Data Types → Imputation → Feature Engineering → Drop Features → Preprocessing → Prediction\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"SOLAR PANEL PREDICTION PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Input raw data shape: {raw_data.shape}\")\n",
    "        \n",
    "        # Step 1: Fix data types (same as training)\n",
    "        print(\"\\nStep 1: Fixing data types...\")\n",
    "        df_fixed = self.fix_data_types(raw_data, \"PREDICTION DATA\")\n",
    "        \n",
    "        # Step 2: Apply imputation pipeline\n",
    "        print(\"\\nStep 2: Applying imputation pipeline...\")\n",
    "        df_imputed = self.apply_imputation(df_fixed)\n",
    "        \n",
    "        # Step 3: Apply feature engineering\n",
    "        print(\"\\nStep 3: Applying feature engineering...\")\n",
    "        df_engineered = self.apply_feature_engineering(df_imputed)\n",
    "        \n",
    "        # Step 4: Drop features\n",
    "        print(\"\\nStep 4: Dropping specified features...\")\n",
    "        df_final = self.drop_features(df_engineered)\n",
    "        \n",
    "        # Step 5: Preprocess features\n",
    "        print(\"\\nStep 5: Preprocessing features...\")\n",
    "        X_processed = self.preprocess_features(df_final)\n",
    "        \n",
    "        # Step 6: Make predictions on transformed scale\n",
    "        print(f\"\\nStep 6: Making predictions...\")\n",
    "        y_pred_transformed = self.best_model.predict(X_processed)\n",
    "        \n",
    "        # Step 7: Transform predictions back to original scale\n",
    "        print(f\"Step 7: Transforming predictions to original scale...\")\n",
    "        y_pred_original = self.inverse_transform_predictions(y_pred_transformed)\n",
    "        \n",
    "        print(f\"✓ Generated {len(y_pred_original)} predictions\")\n",
    "        print(f\"Prediction statistics:\")\n",
    "        print(f\"  Min: {y_pred_original.min():.4f}\")\n",
    "        print(f\"  Max: {y_pred_original.max():.4f}\")\n",
    "        print(f\"  Mean: {y_pred_original.mean():.4f}\")\n",
    "        print(f\"  Std: {y_pred_original.std():.4f}\")\n",
    "        \n",
    "        return y_pred_original\n",
    "    \n",
    "    def predict_with_id(self, raw_data, id_column='id'):\n",
    "        \"\"\"\n",
    "        Make predictions and return with original IDs\n",
    "        \"\"\"\n",
    "        if id_column in raw_data.columns:\n",
    "            ids = raw_data[id_column].copy()\n",
    "            print(f\"Found ID column: {id_column}\")\n",
    "        else:\n",
    "            ids = range(len(raw_data))\n",
    "            print(f\"No ID column found, using sequential IDs\")\n",
    "        \n",
    "        predictions = self.predict(raw_data)\n",
    "        \n",
    "        results = pd.DataFrame({\n",
    "            id_column: ids,\n",
    "            'efficiency': predictions\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_predictions(self, raw_data, output_path='predictions.csv', id_column='id'):\n",
    "        \"\"\"\n",
    "        Generate predictions and save to CSV\n",
    "        \"\"\"\n",
    "        results = self.predict_with_id(raw_data, id_column)\n",
    "        results.to_csv(output_path, index=False)\n",
    "        print(f\"\\n✓ Predictions saved to: {output_path}\")\n",
    "        print(f\"Output format: {list(results.columns)}\")\n",
    "        return results\n",
    "    \n",
    "    def validate_pipeline_compatibility(self):\n",
    "        \"\"\"\n",
    "        Validate that the loaded model has all required components\n",
    "        \"\"\"\n",
    "        required_components = [\n",
    "            'model', 'preprocessor', 'target_transformer', \n",
    "            'feature_names', 'categorical_cols', 'numerical_cols',\n",
    "            'final_feature_names'\n",
    "        ]\n",
    "        \n",
    "        optional_components = ['imputer', 'features_to_drop']\n",
    "        \n",
    "        missing_components = []\n",
    "        for component in required_components:\n",
    "            if component not in self.model_package:\n",
    "                missing_components.append(component)\n",
    "        \n",
    "        if missing_components:\n",
    "            print(f\"Error: Missing required components in saved model: {missing_components}\")\n",
    "            return False\n",
    "        \n",
    "        missing_optional = []\n",
    "        for component in optional_components:\n",
    "            if component not in self.model_package:\n",
    "                missing_optional.append(component)\n",
    "        \n",
    "        if missing_optional:\n",
    "            print(f\"Warning: Missing optional components: {missing_optional}\")\n",
    "            print(\"Pipeline will continue but some features may not be available.\")\n",
    "        \n",
    "        print(\"✓ All required pipeline components are available\")\n",
    "        return True\n",
    "\n",
    "# Utility functions\n",
    "def load_test_data(file_path):\n",
    "    \"\"\"Load test data from CSV\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Test data loaded successfully: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading test data: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize prediction pipeline\n",
    "        print(\"Initializing Solar Panel Prediction Pipeline...\")\n",
    "        pipeline = SolarPanelPredictionPipeline(model_path='')\n",
    "        \n",
    "        # Validate pipeline compatibility\n",
    "        if not pipeline.validate_pipeline_compatibility():\n",
    "            print(\"Warning: Pipeline compatibility issues detected. Proceeding anyway...\")\n",
    "        \n",
    "        # Load test data\n",
    "        print(f\"\\nLoading test data...\")\n",
    "        test_data = load_test_data('../dataset/test.csv')\n",
    "        \n",
    "        # Generate and save predictions\n",
    "        print(f\"\\nStarting prediction process...\")\n",
    "        predictions = pipeline.save_predictions(\n",
    "            raw_data=test_data,\n",
    "            output_path='solar_efficiency_predictions.csv',\n",
    "            id_column='id'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PREDICTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Generated predictions for {len(predictions)} samples\")\n",
    "        print(f\"Results saved to: solar_efficiency_predictions.csv\")\n",
    "        \n",
    "        # Display sample predictions\n",
    "        print(f\"\\nSample predictions:\")\n",
    "        print(predictions.head(10))\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction pipeline: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main prediction pipeline\n",
    "    try:\n",
    "        predictions = main()\n",
    "        print(f\"\\nPipeline executed successfully!\")\n",
    "        print(f\"Total predictions generated: {len(predictions)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
