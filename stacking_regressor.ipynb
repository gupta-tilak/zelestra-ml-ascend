{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed4c2bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌞 SOLAR PANEL PERFORMANCE MODEL SELECTION\n",
      "==================================================\n",
      "🔧 Initializing model selector...\n",
      "🚀 Starting complete pipeline...\n",
      "🚀 STARTING COMPLETE SOLAR PANEL MODEL SELECTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "📊 STEP 1: Data Loading and Preparation\n",
      "Loading raw data...\n",
      "Raw dataset shape: (20000, 17)\n",
      "Missing values in raw data:\n",
      "temperature           1001\n",
      "irradiance             987\n",
      "panel_age             1011\n",
      "maintenance_count     1027\n",
      "soiling_ratio         1010\n",
      "voltage                993\n",
      "current                977\n",
      "module_temperature     978\n",
      "cloud_coverage        1010\n",
      "error_code            5912\n",
      "installation_type     5028\n",
      "dtype: int64\n",
      "\n",
      "Step 1: Fixing data types...\n",
      "\n",
      "=== FIXING DATA TYPES FOR TRAINING DATA ===\n",
      "\n",
      "Processing humidity:\n",
      "Original dtype: object\n",
      "Non-numeric values found in humidity:\n",
      "humidity\n",
      "unknown    50\n",
      "error      40\n",
      "badval     37\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 127\n",
      "Valid numeric values: 19873\n",
      "Min: 0.011\n",
      "Max: 99.995\n",
      "Mean: 50.066\n",
      "\n",
      "Processing wind_speed:\n",
      "Original dtype: object\n",
      "Non-numeric values found in wind_speed:\n",
      "wind_speed\n",
      "badval     42\n",
      "error      41\n",
      "unknown    36\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 119\n",
      "Valid numeric values: 19881\n",
      "Min: 0.001\n",
      "Max: 14.999\n",
      "Mean: 7.413\n",
      "\n",
      "Processing pressure:\n",
      "Original dtype: object\n",
      "Non-numeric values found in pressure:\n",
      "pressure\n",
      "unknown    46\n",
      "error      45\n",
      "badval     44\n",
      "Name: count, dtype: int64\n",
      "Converted dtype: float64\n",
      "Missing values after conversion: 135\n",
      "Valid numeric values: 19865\n",
      "Min: 970.087\n",
      "Max: 1052.866\n",
      "Mean: 1012.981\n",
      "\n",
      "=== DATA TYPE VERIFICATION ===\n",
      "Data types after fixing:\n",
      "humidity: float64\n",
      "wind_speed: float64\n",
      "pressure: float64\n",
      "\n",
      "Step 2: Applying imputation pipeline...\n",
      "Dataset shape after imputation: (20000, 17)\n",
      "Remaining missing values after imputation: 0\n",
      "\n",
      "Step 3: Creating engineered features...\n",
      "Dataset shape after feature engineering: (20000, 31)\n",
      "New features created:\n",
      "['age_category', 'age_degradation_factor', 'effective_module_temp', 'environmental_stress', 'expected_irradiance_clean', 'installation_type_tracking', 'irradiance_cloud_ratio', 'irradiance_normalized', 'maintenance_frequency', 'power_output', 'soiling_loss', 'temp_coefficient_effect', 'temp_difference', 'wind_cooling_effect']\n",
      "\n",
      "Step 4: Dropping features: ['id', 'temperature', 'humidity', 'maintenance_count', 'voltage', 'module_temperature', 'pressure', 'string_id', 'error_code', 'installation_type', 'power_output', 'temp_difference', 'temp_coefficient_effect', 'expected_irradiance_clean', 'age_category', 'environmental_stress', 'effective_module_temp', 'power_output_log', 'temp_difference_robust', 'performance_deviation', 'efficiency_ratio', 'mean', 'std', 'min', 'max', 'power_output_string_mean', 'power_output_string_std', 'power_output_string_min', 'power_output_string_max', 'power_vs_string_mean', 'error_indicator', 'consecutive_errors', 'anomaly_score', 'operating_regime', 'regime_expected_power', 'regime_performance_deviation']\n",
      "Dropped features: ['id', 'temperature', 'humidity', 'maintenance_count', 'voltage', 'module_temperature', 'pressure', 'string_id', 'error_code', 'installation_type', 'power_output', 'temp_difference', 'temp_coefficient_effect', 'expected_irradiance_clean', 'age_category', 'environmental_stress', 'effective_module_temp']\n",
      "Warning: Features not found in dataset: ['power_output_log', 'temp_difference_robust', 'performance_deviation', 'efficiency_ratio', 'mean', 'std', 'min', 'max', 'power_output_string_mean', 'power_output_string_std', 'power_output_string_min', 'power_output_string_max', 'power_vs_string_mean', 'error_indicator', 'consecutive_errors', 'anomaly_score', 'operating_regime', 'regime_expected_power', 'regime_performance_deviation']\n",
      "Dataset shape after dropping features: (20000, 14)\n",
      "Categorical columns: []\n",
      "Numerical columns: ['irradiance', 'panel_age', 'soiling_ratio', 'current', 'cloud_coverage', 'wind_speed', 'irradiance_normalized', 'soiling_loss', 'irradiance_cloud_ratio', 'age_degradation_factor', 'maintenance_frequency', 'wind_cooling_effect', 'installation_type_tracking']\n",
      "Total features: 13\n",
      "\n",
      "🔧 STEP 2: Creating Preprocessing Pipeline\n",
      "Creating preprocessing pipeline...\n",
      "\n",
      "✂️ STEP 3: Train-Test Split\n",
      "Preparing train-test split...\n",
      "Training set shape: (16000, 13)\n",
      "Test set shape: (4000, 13)\n",
      "\n",
      "🤖 STEP 4: Base Model Evaluation\n",
      "Defining models...\n",
      "Evaluating base models...\n",
      "Training Linear Regression...\n",
      "  ✓ Linear Regression completed - Test Custom Score: 89.2358\n",
      "Training Ridge...\n",
      "  ✓ Ridge completed - Test Custom Score: 89.2358\n",
      "Training Lasso...\n",
      "  ✓ Lasso completed - Test Custom Score: 85.7084\n",
      "Training ElasticNet...\n",
      "  ✓ ElasticNet completed - Test Custom Score: 86.4799\n",
      "Training Decision Tree...\n",
      "  ✓ Decision Tree completed - Test Custom Score: 84.3800\n",
      "Training Random Forest...\n",
      "  ✓ Random Forest completed - Test Custom Score: 89.1554\n",
      "Training Extra Trees...\n",
      "  ✓ Extra Trees completed - Test Custom Score: 89.0627\n",
      "Training Gradient Boosting...\n",
      "  ✓ Gradient Boosting completed - Test Custom Score: 89.3384\n",
      "Training XGBoost...\n",
      "  ✓ XGBoost completed - Test Custom Score: 89.0860\n",
      "Training LightGBM...\n",
      "  ✓ LightGBM completed - Test Custom Score: 89.3069\n",
      "Training CatBoost...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1264\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Starting complete pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m best_model, best_model_name, best_score = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🎊 FINAL RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1267\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1200\u001b[39m, in \u001b[36mSolarPanelModelSelector.run_complete_pipeline\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1198\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🤖 STEP 4: Base Model Evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1199\u001b[39m \u001b[38;5;28mself\u001b[39m.define_models()\n\u001b[32m-> \u001b[39m\u001b[32m1200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_base_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;66;03m# Step 5: Hyperparameter tuning\u001b[39;00m\n\u001b[32m   1203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⚙️ STEP 5: Hyperparameter Tuning\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 442\u001b[39m, in \u001b[36mSolarPanelModelSelector.evaluate_base_models\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m     cv_scores = cross_val_score(model, \u001b[38;5;28mself\u001b[39m.X_train, \u001b[38;5;28mself\u001b[39m.y_train, cv=\u001b[32m3\u001b[39m, \n\u001b[32m    440\u001b[39m                               scoring=\u001b[33m'\u001b[39m\u001b[33mneg_mean_squared_error\u001b[39m\u001b[33m'\u001b[39m, n_jobs=\u001b[32m1\u001b[39m)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     cv_scores = \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mneg_mean_squared_error\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# Fit model for additional metrics\u001b[39;00m\n\u001b[32m    446\u001b[39m model.fit(\u001b[38;5;28mself\u001b[39m.X_train, \u001b[38;5;28mself\u001b[39m.y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:411\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Zelestra-ML-Ascend/venv/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Neural Network imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from utils.imputation import ImputationPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class ANNRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Custom ANN Regressor wrapper that mimics scikit-learn interface\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons=128, layers=3, dropout_rate=0.3, \n",
    "                 learning_rate=0.001, l1_reg=0.0, l2_reg=0.01,\n",
    "                 epochs=200, batch_size=32, validation_split=0.2,\n",
    "                 patience=20, verbose=0):\n",
    "        self.neurons = neurons\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "        self.history_ = None\n",
    "        \n",
    "    def _build_model(self, input_dim):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(Dense(self.neurons, \n",
    "                       input_dim=input_dim,\n",
    "                       activation='relu',\n",
    "                       kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(self.layers - 1):\n",
    "            # Gradually decrease neurons in deeper layers\n",
    "            layer_neurons = max(self.neurons // (2 ** i), 32)\n",
    "            model.add(Dense(layer_neurons,\n",
    "                           activation='relu',\n",
    "                           kernel_regularizer=l1_l2(l1=self.l1_reg, l2=self.l2_reg)))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\"Fit the neural network\"\"\"\n",
    "        # Convert to numpy arrays if needed\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Ensure y is 1D\n",
    "        if len(y.shape) > 1:\n",
    "            y = y.flatten()\n",
    "        \n",
    "        # Build model\n",
    "        self.model_ = self._build_model(X.shape[1])\n",
    "        \n",
    "        # Set up callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=self.patience, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(patience=self.patience//2, factor=0.5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history_ = self.model_.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            validation_split=self.validation_split,\n",
    "            callbacks=callbacks,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        # Convert to numpy array if needed\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        predictions = self.model_.predict(X, verbose=0)\n",
    "        return predictions.flatten()\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator\"\"\"\n",
    "        return {\n",
    "            'neurons': self.neurons,\n",
    "            'layers': self.layers,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'l1_reg': self.l1_reg,\n",
    "            'l2_reg': self.l2_reg,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'validation_split': self.validation_split,\n",
    "            'patience': self.patience,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set parameters for this estimator\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "class SolarPanelModelSelector:\n",
    "    def __init__(self, data_path='dataset/train.csv', test_size=0.2, random_state=42, features_to_drop=None):\n",
    "        \"\"\"\n",
    "        Initialize the model selector with data loading and basic setup\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.preprocessor = None\n",
    "        self.imputer = None\n",
    "        self.features_to_drop = features_to_drop or []\n",
    "        \n",
    "        # Stacking-specific attributes\n",
    "        self.stacking_results = {}\n",
    "        self.best_stacking_model = None\n",
    "        \n",
    "    def fix_data_types(self, df, dataset_name):\n",
    "        \"\"\"Fix data type inconsistencies for specific columns\"\"\"\n",
    "        df_fixed = df.copy()\n",
    "        \n",
    "        # Define columns that should be numeric\n",
    "        numeric_columns_to_fix = ['humidity', 'wind_speed', 'pressure']\n",
    "        \n",
    "        print(f\"\\n=== FIXING DATA TYPES FOR {dataset_name} ===\")\n",
    "        \n",
    "        for col in numeric_columns_to_fix:\n",
    "            if col in df_fixed.columns:\n",
    "                print(f\"\\nProcessing {col}:\")\n",
    "                print(f\"Original dtype: {df_fixed[col].dtype}\")\n",
    "                \n",
    "                # Check for non-numeric values before conversion\n",
    "                if df_fixed[col].dtype == 'object':\n",
    "                    # Display unique non-numeric values\n",
    "                    try:\n",
    "                        # Try to convert to numeric and see what fails\n",
    "                        numeric_conversion = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        non_numeric_mask = pd.isna(numeric_conversion) & df_fixed[col].notna()\n",
    "                        \n",
    "                        if non_numeric_mask.any():\n",
    "                            print(f\"Non-numeric values found in {col}:\")\n",
    "                            non_numeric_values = df_fixed.loc[non_numeric_mask, col].value_counts()\n",
    "                            print(non_numeric_values.head(10))\n",
    "                            \n",
    "                            # Handle common non-numeric patterns\n",
    "                            df_fixed[col] = df_fixed[col].astype(str)\n",
    "                            \n",
    "                            # Remove common problematic characters\n",
    "                            df_fixed[col] = df_fixed[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "                            df_fixed[col] = df_fixed[col].str.strip()\n",
    "                            \n",
    "                            # Handle empty strings\n",
    "                            df_fixed[col] = df_fixed[col].replace('', np.nan)\n",
    "                            df_fixed[col] = df_fixed[col].replace('nan', np.nan)\n",
    "                            \n",
    "                        # Convert to numeric\n",
    "                        df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "                        \n",
    "                        print(f\"Converted dtype: {df_fixed[col].dtype}\")\n",
    "                        print(f\"Missing values after conversion: {df_fixed[col].isnull().sum()}\")\n",
    "                        print(f\"Valid numeric values: {df_fixed[col].notna().sum()}\")\n",
    "                        \n",
    "                        # Basic statistics for converted column\n",
    "                        if df_fixed[col].notna().any():\n",
    "                            print(f\"Min: {df_fixed[col].min():.3f}\")\n",
    "                            print(f\"Max: {df_fixed[col].max():.3f}\")\n",
    "                            print(f\"Mean: {df_fixed[col].mean():.3f}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error converting {col}: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"{col} is already numeric type: {df_fixed[col].dtype}\")\n",
    "        \n",
    "        return df_fixed\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data, fix data types, and apply imputation pipeline\n",
    "        \"\"\"\n",
    "        print(\"Loading raw data...\")\n",
    "        self.df_raw = pd.read_csv(self.data_path)\n",
    "        print(f\"Raw dataset shape: {self.df_raw.shape}\")\n",
    "        print(f\"Missing values in raw data:\\n{self.df_raw.isnull().sum()[self.df_raw.isnull().sum() > 0]}\")\n",
    "        \n",
    "        # Step 1: Fix data types BEFORE imputation\n",
    "        print(\"\\nStep 1: Fixing data types...\")\n",
    "        self.df_fixed = self.fix_data_types(self.df_raw, \"TRAINING DATA\")\n",
    "        \n",
    "        # Verify the fixes\n",
    "        print(\"\\n=== DATA TYPE VERIFICATION ===\")\n",
    "        print(\"Data types after fixing:\")\n",
    "        for col in ['humidity', 'wind_speed', 'pressure']:\n",
    "            if col in self.df_fixed.columns:\n",
    "                print(f\"{col}: {self.df_fixed[col].dtype}\")\n",
    "        \n",
    "        # Step 2: Initialize and apply imputation pipeline (WITHOUT feature creation)\n",
    "        print(\"\\nStep 2: Applying imputation pipeline...\")\n",
    "        self.imputer = ImputationPipeline()\n",
    "        self.df_imputed = self.imputer.fit_transform(self.df_fixed)\n",
    "        \n",
    "        print(f\"Dataset shape after imputation: {self.df_imputed.shape}\")\n",
    "        remaining_missing = self.df_imputed.isnull().sum().sum()\n",
    "        print(f\"Remaining missing values after imputation: {remaining_missing}\")\n",
    "        \n",
    "        # Step 3: NOW create features AFTER imputation is complete\n",
    "        print(\"\\nStep 3: Creating engineered features...\")\n",
    "        from utils.feature_engineering import SolarFeatureEngineering\n",
    "        feature_engineer = SolarFeatureEngineering()\n",
    "        self.df = feature_engineer.create_solar_features(self.df_imputed)\n",
    "        print(f\"Dataset shape after feature engineering: {self.df.shape}\")\n",
    "        print(\"New features created:\")\n",
    "        print(self.df.columns.difference(self.df_imputed.columns).tolist())\n",
    "        \n",
    "        # Verify no missing values in new features\n",
    "        new_missing = self.df.isnull().sum().sum()\n",
    "        if new_missing > 0:\n",
    "            print(f\"Warning: {new_missing} missing values found after feature engineering\")\n",
    "            print(\"Missing values by column:\")\n",
    "            print(self.df.isnull().sum()[self.df.isnull().sum() > 0])\n",
    "\n",
    "        # Step 4: Drop selected features if specified\n",
    "        if self.features_to_drop:\n",
    "            print(f\"\\nStep 4: Dropping features: {self.features_to_drop}\")\n",
    "            available_features = [col for col in self.features_to_drop if col in self.df.columns]\n",
    "            unavailable_features = [col for col in self.features_to_drop if col not in self.df.columns]\n",
    "            \n",
    "            if available_features:\n",
    "                self.df = self.df.drop(columns=available_features)\n",
    "                print(f\"Dropped features: {available_features}\")\n",
    "            \n",
    "            if unavailable_features:\n",
    "                print(f\"Warning: Features not found in dataset: {unavailable_features}\")\n",
    "            \n",
    "            print(f\"Dataset shape after dropping features: {self.df.shape}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        self.target_col = 'efficiency'\n",
    "        self.feature_cols = [col for col in self.df.columns if col != self.target_col]\n",
    "        \n",
    "        # Identify categorical and numerical columns\n",
    "        self.categorical_cols = self.df[self.feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "        self.numerical_cols = self.df[self.feature_cols].select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        \n",
    "        print(f\"Categorical columns: {self.categorical_cols}\")\n",
    "        print(f\"Numerical columns: {self.numerical_cols}\")\n",
    "        print(f\"Total features: {len(self.feature_cols)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create preprocessing pipeline for numerical and categorical features\n",
    "        \"\"\"\n",
    "        print(\"Creating preprocessing pipeline...\")\n",
    "        \n",
    "        # For neural networks, we need StandardScaler instead of RobustScaler\n",
    "        # as neural networks work better with standardized inputs\n",
    "        numerical_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler())  # Neural networks prefer StandardScaler\n",
    "        ])\n",
    "        \n",
    "        # Categorical preprocessing pipeline\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('encoder', 'passthrough')  # Will be handled separately\n",
    "        ])\n",
    "        \n",
    "        # Create preprocessor\n",
    "        self.preprocessor = ColumnTransformer([\n",
    "            ('num', numerical_pipeline, self.numerical_cols),\n",
    "            ('cat', categorical_pipeline, self.categorical_cols)\n",
    "        ])\n",
    "        \n",
    "        return self.preprocessor\n",
    "    \n",
    "    def prepare_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Prepare train-test split with proper preprocessing\n",
    "        \"\"\"\n",
    "        print(\"Preparing train-test split...\")\n",
    "        \n",
    "        X = self.df[self.feature_cols].copy()\n",
    "        y = self.df[self.target_col].copy()\n",
    "        \n",
    "        # Store original target values for later use\n",
    "        self.y_original = y.copy()\n",
    "        \n",
    "        # Apply power transformation to target if it's skewed\n",
    "        self.target_transformer = PowerTransformer(method='yeo-johnson')\n",
    "        y_transformed = self.target_transformer.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y_transformed, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Also split original target for evaluation\n",
    "        _, _, self.y_train_original, self.y_test_original = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=None\n",
    "        )\n",
    "        \n",
    "        # Handle categorical encoding\n",
    "        self.label_encoders = {}\n",
    "        for col in self.categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "            X_test[col] = le.transform(X_test[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        \n",
    "        # Apply numerical preprocessing\n",
    "        X_train_scaled = self.preprocessor.fit_transform(X_train)\n",
    "        X_test_scaled = self.preprocessor.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "        \n",
    "        self.X_train, self.X_test = X_train_scaled, X_test_scaled\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        \n",
    "        print(f\"Training set shape: {self.X_train.shape}\")\n",
    "        print(f\"Test set shape: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "    \n",
    "    def define_models(self):\n",
    "        \"\"\"\n",
    "        Define all models to be tested including ANN\n",
    "        \"\"\"\n",
    "        print(\"Defining models...\")\n",
    "        \n",
    "        self.models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(random_state=self.random_state),\n",
    "            'Lasso': Lasso(random_state=self.random_state),\n",
    "            'ElasticNet': ElasticNet(random_state=self.random_state),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=self.random_state),\n",
    "            'Random Forest': RandomForestRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Extra Trees': ExtraTreesRegressor(random_state=self.random_state, n_jobs=-1),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(random_state=self.random_state),\n",
    "            'XGBoost': XGBRegressor(random_state=self.random_state, eval_metric='rmse'),\n",
    "            'LightGBM': LGBMRegressor(random_state=self.random_state, verbose=-1),\n",
    "            'CatBoost': CatBoostRegressor(random_state=self.random_state, verbose=False),\n",
    "            'KNN': KNeighborsRegressor(),\n",
    "            'SVR': SVR(),\n",
    "            # 'ANN': ANNRegressor(verbose=0)  # Use custom ANN wrapper\n",
    "        }\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_pred):\n",
    "        \"\"\"\n",
    "        Apply inverse transformation to predictions to get them back to original scale\n",
    "        \"\"\"\n",
    "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "        y_pred_original = self.target_transformer.inverse_transform(y_pred_reshaped).flatten()\n",
    "        return y_pred_original\n",
    "    \n",
    "    def custom_score_function(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Custom scoring function as per problem statement\n",
    "        Score = 100*(1-sqrt(MSE))\n",
    "        Note: This should be calculated on original scale, not transformed scale\n",
    "        \"\"\"\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        score = 100 * (1 - np.sqrt(mse))\n",
    "        return score\n",
    "    \n",
    "    def evaluate_base_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate all base models using cross-validation\n",
    "        \"\"\"\n",
    "        print(\"Evaluating base models...\")\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Cross-validation scores (on transformed target)\n",
    "                if name == 'ANN':\n",
    "                    # For ANN, use fewer CV folds due to computational cost\n",
    "                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3, \n",
    "                                              scoring='neg_mean_squared_error', n_jobs=1)\n",
    "                else:\n",
    "                    cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, \n",
    "                                              scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "                \n",
    "                # Fit model for additional metrics\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                \n",
    "                # Get predictions on transformed scale\n",
    "                y_pred_train_transformed = model.predict(self.X_train)\n",
    "                y_pred_test_transformed = model.predict(self.X_test)\n",
    "                \n",
    "                # Transform predictions back to original scale\n",
    "                y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "                \n",
    "                # Calculate metrics on ORIGINAL scale\n",
    "                train_rmse = np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original))\n",
    "                test_rmse = np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original))\n",
    "                train_r2 = r2_score(self.y_train_original, y_pred_train_original)\n",
    "                test_r2 = r2_score(self.y_test_original, y_pred_test_original)\n",
    "                \n",
    "                # Custom score on original scale\n",
    "                train_custom_score = self.custom_score_function(self.y_train_original, y_pred_train_original)\n",
    "                test_custom_score = self.custom_score_function(self.y_test_original, y_pred_test_original)\n",
    "                \n",
    "                # CV RMSE on transformed scale (for comparison)\n",
    "                cv_rmse_transformed = np.sqrt(-cv_scores.mean())\n",
    "                \n",
    "                self.results[name] = {\n",
    "                    'CV_RMSE_transformed': cv_rmse_transformed,\n",
    "                    'CV_RMSE_std': np.sqrt(cv_scores.std()),\n",
    "                    'Train_RMSE': train_rmse,\n",
    "                    'Test_RMSE': test_rmse,\n",
    "                    'Train_R2': train_r2,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Train_Custom_Score': train_custom_score,\n",
    "                    'Test_Custom_Score': test_custom_score,\n",
    "                    'Model': model\n",
    "                }\n",
    "                \n",
    "                print(f\"  ✓ {name} completed - Test Custom Score: {test_custom_score:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error training {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_hyperparameter_grids(self):\n",
    "        \"\"\"\n",
    "        Define hyperparameter grids for top performing models including ANN\n",
    "        \"\"\"\n",
    "        param_grids = {\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2']\n",
    "            },\n",
    "            'XGBoost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'LightGBM': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'Ridge': {\n",
    "                'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "            },\n",
    "            'Lasso': {\n",
    "                'alpha': [0.001, 0.01, 0.1, 1.0]\n",
    "            },\n",
    "            'ANN': {\n",
    "                'neurons': [64, 128, 256],\n",
    "                'layers': [2, 3, 4],\n",
    "                'dropout_rate': [0.2, 0.3, 0.4],\n",
    "                'learning_rate': [0.001, 0.01],\n",
    "                'l2_reg': [0.001, 0.01, 0.1],\n",
    "                'epochs': [150, 250],\n",
    "                'batch_size': [16, 32, 64]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return param_grids\n",
    "    \n",
    "    def hyperparameter_tuning(self, top_n=5):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning for top N models including ANN\n",
    "        \"\"\"\n",
    "        print(f\"Performing hyperparameter tuning for top {top_n} models...\")\n",
    "        \n",
    "        # Sort models by test RMSE\n",
    "        sorted_models = sorted(self.results.items(), key=lambda x: x[1]['Test_RMSE'])\n",
    "        top_models = [name for name, _ in sorted_models[:top_n]]\n",
    "        \n",
    "        param_grids = self.get_hyperparameter_grids()\n",
    "        tuned_results = {}\n",
    "        \n",
    "        for model_name in top_models:\n",
    "            if model_name in param_grids:\n",
    "                print(f\"Tuning {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    base_model = self.models[model_name]\n",
    "                    param_grid = param_grids[model_name]\n",
    "                    \n",
    "                    # Use different search strategies for different models\n",
    "                    if model_name == 'ANN':\n",
    "                        # Use fewer iterations for ANN due to computational cost\n",
    "                        grid_search = RandomizedSearchCV(\n",
    "                            base_model, param_grid, n_iter=10, cv=3,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=1,\n",
    "                            random_state=self.random_state\n",
    "                        )\n",
    "                    else:\n",
    "                        # Use RandomizedSearchCV for faster tuning\n",
    "                        grid_search = RandomizedSearchCV(\n",
    "                            base_model, param_grid, n_iter=20, cv=5,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=-1,\n",
    "                            random_state=self.random_state\n",
    "                        )\n",
    "                    \n",
    "                    grid_search.fit(self.X_train, self.y_train)\n",
    "                    \n",
    "                    # Evaluate best model\n",
    "                    best_model = grid_search.best_estimator_\n",
    "                    \n",
    "                    # Get predictions on transformed scale\n",
    "                    y_pred_train_transformed = best_model.predict(self.X_train)\n",
    "                    y_pred_test_transformed = best_model.predict(self.X_test)\n",
    "                    \n",
    "                    # Transform predictions back to original scale\n",
    "                    y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                    y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "                    \n",
    "                    tuned_results[f'{model_name}_Tuned'] = {\n",
    "                        'Best_Params': grid_search.best_params_,\n",
    "                        'CV_RMSE_transformed': np.sqrt(-grid_search.best_score_),\n",
    "                        'Train_RMSE': np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original)),\n",
    "                        'Test_RMSE': np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original)),\n",
    "                        'Train_R2': r2_score(self.y_train_original, y_pred_train_original),\n",
    "                        'Test_R2': r2_score(self.y_test_original, y_pred_test_original),\n",
    "                        'Train_Custom_Score': self.custom_score_function(self.y_train_original, y_pred_train_original),\n",
    "                        'Test_Custom_Score': self.custom_score_function(self.y_test_original, y_pred_test_original),\n",
    "                        'Model': best_model\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  ✓ {model_name} tuning completed\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error tuning {model_name}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        self.tuned_results = tuned_results\n",
    "        return tuned_results\n",
    "    \n",
    "    def create_stacking_models(self, n_best=2):\n",
    "        \"\"\"\n",
    "        Create stacking regressor using the top N performing tuned models as base estimators.\n",
    "        \n",
    "        Parameters:\n",
    "        n_best: Number of best models to use as base estimators (default: 2)\n",
    "        \"\"\"\n",
    "        print(f\"\\nCreating stacking regressor with top {n_best} tuned models...\")\n",
    "\n",
    "        if not hasattr(self, 'tuned_results') or not self.tuned_results:\n",
    "            raise ValueError(\"No tuned models found. Please tune models before creating stacking models.\")\n",
    "\n",
    "        # Step 1: Create a dictionary of tuned models only\n",
    "        best_models_by_type = {}\n",
    "\n",
    "        for name, results in self.tuned_results.items():\n",
    "            # Extract the base model type from tuned name (e.g., \"ANN_Tuned\" -> \"ann\")\n",
    "            if name.endswith('_Tuned'):\n",
    "                base_name = name[:-6]  # Remove \"_Tuned\"\n",
    "            else:\n",
    "                base_name = name\n",
    "            \n",
    "            model_type = base_name.lower().replace(' ', '_')\n",
    "            best_models_by_type[model_type] = {\n",
    "                'name': name,\n",
    "                'results': results,\n",
    "                'is_tuned': True\n",
    "            }\n",
    "\n",
    "        # Step 2: Sort tuned models by performance\n",
    "        sorted_models = sorted(\n",
    "            best_models_by_type.items(),\n",
    "            key=lambda x: x[1]['results']['Test_Custom_Score'],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        top_models = sorted_models[:n_best]\n",
    "\n",
    "        print(f\"Selected base estimators for stacking (tuned only):\")\n",
    "        for i, (model_type, model_info) in enumerate(top_models, 1):\n",
    "            print(f\"  {i}. {model_info['name']} - Test Custom Score: {model_info['results']['Test_Custom_Score']:.4f}\")\n",
    "\n",
    "        # Step 3: Prepare base estimators\n",
    "        base_estimators = []\n",
    "        for model_type, model_info in top_models:\n",
    "            estimator = model_info['results']['Model']\n",
    "            base_estimators.append((model_type, estimator))\n",
    "\n",
    "        estimator_names = [name for name, _ in base_estimators]\n",
    "        if len(estimator_names) != len(set(estimator_names)):\n",
    "            raise ValueError(f\"Duplicate estimator names found: {estimator_names}\")\n",
    "\n",
    "        print(f\"Base estimator names: {estimator_names}\")\n",
    "\n",
    "        # Step 4: Define meta-regressors\n",
    "        meta_regressors = {\n",
    "            'Linear': LinearRegression(),\n",
    "            'Ridge': Ridge(alpha=1.0, random_state=self.random_state),\n",
    "            'Lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, random_state=self.random_state),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=self.random_state, n_jobs=-1)\n",
    "        }\n",
    "\n",
    "        stacking_models = {}\n",
    "\n",
    "        for meta_name, meta_regressor in meta_regressors.items():\n",
    "            stacking_name = f\"Stacking_{meta_name}\"\n",
    "\n",
    "            stacking_model = StackingRegressor(\n",
    "                estimators=base_estimators,\n",
    "                final_estimator=meta_regressor,\n",
    "                cv=3,\n",
    "                n_jobs=-1,\n",
    "                passthrough=False\n",
    "            )\n",
    "\n",
    "            stacking_models[stacking_name] = stacking_model\n",
    "\n",
    "        # Step 5: Train and evaluate stacking models\n",
    "        self.stacking_results = {}\n",
    "\n",
    "        for stacking_name, stacking_model in stacking_models.items():\n",
    "            print(f\"\\nTraining {stacking_name}...\")\n",
    "\n",
    "            try:\n",
    "                stacking_model.fit(self.X_train, self.y_train)\n",
    "\n",
    "                y_pred_train_transformed = stacking_model.predict(self.X_train)\n",
    "                y_pred_test_transformed = stacking_model.predict(self.X_test)\n",
    "\n",
    "                y_pred_train_original = self.inverse_transform_predictions(y_pred_train_transformed)\n",
    "                y_pred_test_original = self.inverse_transform_predictions(y_pred_test_transformed)\n",
    "\n",
    "                train_rmse = np.sqrt(mean_squared_error(self.y_train_original, y_pred_train_original))\n",
    "                test_rmse = np.sqrt(mean_squared_error(self.y_test_original, y_pred_test_original))\n",
    "                train_r2 = r2_score(self.y_train_original, y_pred_train_original)\n",
    "                test_r2 = r2_score(self.y_test_original, y_pred_test_original)\n",
    "\n",
    "                train_custom_score = self.custom_score_function(self.y_train_original, y_pred_train_original)\n",
    "                test_custom_score = self.custom_score_function(self.y_test_original, y_pred_test_original)\n",
    "\n",
    "                cv_scores = cross_val_score(stacking_model, self.X_train, self.y_train,\n",
    "                                            cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "                cv_rmse_transformed = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "                base_estimator_info = []\n",
    "                for model_type, model_info in top_models:\n",
    "                    base_estimator_info.append({\n",
    "                        'name': model_info['name'],\n",
    "                        'type': model_type,\n",
    "                        'is_tuned': True,\n",
    "                        'score': model_info['results']['Test_Custom_Score']\n",
    "                    })\n",
    "\n",
    "                self.stacking_results[stacking_name] = {\n",
    "                    'Base_Estimators': base_estimator_info,\n",
    "                    'Meta_Regressor': meta_name,\n",
    "                    'CV_RMSE_transformed': cv_rmse_transformed,\n",
    "                    'CV_RMSE_std': np.sqrt(cv_scores.std()),\n",
    "                    'Train_RMSE': train_rmse,\n",
    "                    'Test_RMSE': test_rmse,\n",
    "                    'Train_R2': train_r2,\n",
    "                    'Test_R2': test_r2,\n",
    "                    'Train_Custom_Score': train_custom_score,\n",
    "                    'Test_Custom_Score': test_custom_score,\n",
    "                    'Model': stacking_model\n",
    "                }\n",
    "\n",
    "                print(f\"  ✓ {stacking_name} completed - Test Custom Score: {test_custom_score:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error training {stacking_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Step 6: Select best stacking model\n",
    "        if self.stacking_results:\n",
    "            best_stacking_name = max(self.stacking_results.keys(),\n",
    "                                    key=lambda x: self.stacking_results[x]['Test_Custom_Score'])\n",
    "            self.best_stacking_model = self.stacking_results[best_stacking_name]['Model']\n",
    "\n",
    "            print(f\"\\nBest stacking model: {best_stacking_name}\")\n",
    "            print(f\"Test Custom Score: {self.stacking_results[best_stacking_name]['Test_Custom_Score']:.4f}\")\n",
    "            print(f\"Test RMSE: {self.stacking_results[best_stacking_name]['Test_RMSE']:.4f}\")\n",
    "            print(f\"Test R²: {self.stacking_results[best_stacking_name]['Test_R2']:.4f}\")\n",
    "\n",
    "            print(\"Base estimators used:\")\n",
    "            for estimator_info in self.stacking_results[best_stacking_name]['Base_Estimators']:\n",
    "                print(f\"  - {estimator_info['name']} (Tuned): {estimator_info['score']:.4f}\")\n",
    "\n",
    "        return self.stacking_results\n",
    "\n",
    "\n",
    "    def compare_all_models(self):\n",
    "        \"\"\"\n",
    "        Compare all models including base, tuned, and stacking models\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Combine all results\n",
    "        all_model_results = {}\n",
    "        \n",
    "        # Add base model results with \"Base_\" prefix for clarity\n",
    "        for name, results in self.results.items():\n",
    "            all_model_results[f\"Base_{name}\"] = results\n",
    "        \n",
    "        # Add tuned model results (these already have clear names)\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            for name, results in self.tuned_results.items():\n",
    "                all_model_results[name] = results\n",
    "        \n",
    "        # Add stacking model results\n",
    "        if hasattr(self, 'stacking_results'):\n",
    "            for name, results in self.stacking_results.items():\n",
    "                all_model_results[name] = results\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for name, results in all_model_results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'Test_RMSE': results['Test_RMSE'],\n",
    "                'Test_R2': results['Test_R2'],\n",
    "                'Test_Custom_Score': results['Test_Custom_Score'],\n",
    "                'Train_Custom_Score': results['Train_Custom_Score'],\n",
    "                'Overfitting': results['Train_Custom_Score'] - results['Test_Custom_Score']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df = comparison_df.sort_values('Test_Custom_Score', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Models by Test Custom Score:\")\n",
    "        print(\"-\" * 95)\n",
    "        print(f\"{'Model':<30} {'Test_RMSE':<12} {'Test_R²':<10} {'Test_Score':<12} {'Overfitting':<12} {'Type':<15}\")\n",
    "        print(\"-\" * 95)\n",
    "        \n",
    "        for _, row in comparison_df.head(10).iterrows():\n",
    "            # Determine model type\n",
    "            model_name = row['Model']\n",
    "            if model_name.startswith('Base_'):\n",
    "                model_type = 'Base'\n",
    "            elif model_name.endswith('_Tuned'):\n",
    "                model_type = 'Tuned'\n",
    "            elif model_name.startswith('Stacking_'):\n",
    "                model_type = 'Stacking'\n",
    "            else:\n",
    "                model_type = 'Other'\n",
    "            \n",
    "            print(f\"{model_name:<30} {row['Test_RMSE']:<12.4f} {row['Test_R2']:<10.4f} \"\n",
    "                f\"{row['Test_Custom_Score']:<12.4f} {row['Overfitting']:<12.4f} {model_type:<15}\")\n",
    "        \n",
    "        # Identify the overall best model\n",
    "        best_model_name = comparison_df.iloc[0]['Model']\n",
    "        best_model_results = all_model_results[best_model_name]\n",
    "        \n",
    "        print(f\"\\n🏆 BEST OVERALL MODEL: {best_model_name}\")\n",
    "        print(f\"   Test Custom Score: {best_model_results['Test_Custom_Score']:.4f}\")\n",
    "        print(f\"   Test RMSE: {best_model_results['Test_RMSE']:.4f}\")\n",
    "        print(f\"   Test R²: {best_model_results['Test_R2']:.4f}\")\n",
    "        \n",
    "        # Show additional information for stacking models\n",
    "        if best_model_name.startswith('Stacking_') and hasattr(self, 'stacking_results'):\n",
    "            stacking_info = self.stacking_results[best_model_name]\n",
    "            print(f\"   Meta Regressor: {stacking_info['Meta_Regressor']}\")\n",
    "            print(f\"   Base Estimators:\")\n",
    "            for estimator_info in stacking_info['Base_Estimators']:\n",
    "                status = \"Tuned\" if estimator_info['is_tuned'] else \"Base\"\n",
    "                print(f\"     - {estimator_info['name']} ({status}): {estimator_info['score']:.4f}\")\n",
    "        \n",
    "        # Store the best model\n",
    "        self.best_model = best_model_results['Model']\n",
    "        self.best_model_name = best_model_name\n",
    "        \n",
    "        return comparison_df\n",
    "\n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        Get a comprehensive summary of the model selection process\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'dataset_info': {\n",
    "                'original_shape': self.df_raw.shape if hasattr(self, 'df_raw') else None,\n",
    "                'final_shape': self.df.shape if hasattr(self, 'df') else None,\n",
    "                'features_dropped': self.features_to_drop,\n",
    "                'categorical_features': len(self.categorical_cols) if hasattr(self, 'categorical_cols') else 0,\n",
    "                'numerical_features': len(self.numerical_cols) if hasattr(self, 'numerical_cols') else 0\n",
    "            },\n",
    "            'model_counts': {\n",
    "                'base_models': len(self.results) if hasattr(self, 'results') else 0,\n",
    "                'tuned_models': len(self.tuned_results) if hasattr(self, 'tuned_results') else 0,\n",
    "                'stacking_models': len(self.stacking_results) if hasattr(self, 'stacking_results') else 0\n",
    "            },\n",
    "            'best_model': {\n",
    "                'name': self.best_model_name if hasattr(self, 'best_model_name') else None,\n",
    "                'test_custom_score': None,\n",
    "                'test_rmse': None,\n",
    "                'test_r2': None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add best model metrics if available\n",
    "        if hasattr(self, 'best_model_name'):\n",
    "            # Find the best model results\n",
    "            all_results = {**self.results}\n",
    "            if hasattr(self, 'tuned_results'):\n",
    "                all_results.update(self.tuned_results)\n",
    "            if hasattr(self, 'stacking_results'):\n",
    "                all_results.update(self.stacking_results)\n",
    "            \n",
    "            if self.best_model_name in all_results:\n",
    "                best_results = all_results[self.best_model_name]\n",
    "                summary['best_model'].update({\n",
    "                    'test_custom_score': best_results['Test_Custom_Score'],\n",
    "                    'test_rmse': best_results['Test_RMSE'],\n",
    "                    'test_r2': best_results['Test_R2']\n",
    "                })\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete model selection pipeline including stacking\n",
    "        \"\"\"\n",
    "        print(\"🚀 STARTING COMPLETE MODEL SELECTION PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load and prepare data\n",
    "        print(\"\\n📊 STEP 1: Data Loading and Preparation\")\n",
    "        self.load_and_prepare_data()\n",
    "        \n",
    "        # Step 2: Create preprocessing pipeline\n",
    "        print(\"\\n🔧 STEP 2: Creating Preprocessing Pipeline\")\n",
    "        self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Step 3: Prepare train-test split\n",
    "        print(\"\\n✂️ STEP 3: Train-Test Split\")\n",
    "        self.prepare_train_test_split()\n",
    "        \n",
    "        # Step 4: Define and evaluate base models\n",
    "        print(\"\\n🤖 STEP 4: Base Model Evaluation\")\n",
    "        self.define_models()\n",
    "        self.evaluate_base_models()\n",
    "        \n",
    "        # Step 5: Hyperparameter tuning\n",
    "        print(\"\\n⚙️ STEP 5: Hyperparameter Tuning\")\n",
    "        self.hyperparameter_tuning(top_n=5)\n",
    "        \n",
    "        # Step 6: Create stacking models\n",
    "        print(\"\\n🏗️ STEP 6: Stacking Model Creation\")\n",
    "        self.create_stacking_models(n_best=2)\n",
    "        \n",
    "        # Step 7: Compare all models\n",
    "        print(\"\\n📈 STEP 7: Final Model Comparison\")\n",
    "        comparison_df = self.compare_all_models()\n",
    "        \n",
    "        # Step 8: Generate summary\n",
    "        print(\"\\n📋 STEP 8: Pipeline Summary\")\n",
    "        summary = self.get_model_summary()\n",
    "        \n",
    "        print(\"\\n✅ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"Best Model: {summary['best_model']['name']}\")\n",
    "        print(f\"Final Test Score: {summary['best_model']['test_custom_score']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'comparison_df': comparison_df,\n",
    "            'summary': summary,\n",
    "            'best_model': self.best_model,\n",
    "            'best_model_name': self.best_model_name\n",
    "        }\n",
    "    \n",
    "    def select_best_model(self):\n",
    "        \"\"\"\n",
    "        Select the best model from all evaluated models (base, tuned, and stacking)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SELECTING BEST MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Combine all results\n",
    "        all_results = {}\n",
    "        \n",
    "        # Add base model results\n",
    "        if hasattr(self, 'results'):\n",
    "            for name, results in self.results.items():\n",
    "                all_results[f\"Base_{name}\"] = results\n",
    "        \n",
    "        # Add tuned model results\n",
    "        if hasattr(self, 'tuned_results'):\n",
    "            for name, results in self.tuned_results.items():\n",
    "                all_results[name] = results\n",
    "        \n",
    "        # Add stacking model results\n",
    "        if hasattr(self, 'stacking_results'):\n",
    "            for name, results in self.stacking_results.items():\n",
    "                all_results[name] = results\n",
    "        \n",
    "        if not all_results:\n",
    "            raise ValueError(\"No models have been evaluated yet!\")\n",
    "        \n",
    "        # Find best model based on Test Custom Score\n",
    "        best_model_name = max(all_results.keys(), \n",
    "                            key=lambda x: all_results[x]['Test_Custom_Score'])\n",
    "        \n",
    "        best_results = all_results[best_model_name]\n",
    "        \n",
    "        # Store best model information\n",
    "        self.best_model = best_results['Model']\n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_score = best_results['Test_Custom_Score']\n",
    "        self.best_results = best_results\n",
    "        \n",
    "        print(f\"🏆 BEST MODEL SELECTED: {best_model_name}\")\n",
    "        print(f\"   Test Custom Score: {self.best_score:.4f}\")\n",
    "        print(f\"   Test RMSE: {best_results['Test_RMSE']:.4f}\")\n",
    "        print(f\"   Test R²: {best_results['Test_R2']:.4f}\")\n",
    "        \n",
    "        # Display model-specific information\n",
    "        if 'Best_Params' in best_results:\n",
    "            print(f\"   Best Parameters: {best_results['Best_Params']}\")\n",
    "        elif 'Base_Estimators' in best_results:\n",
    "            print(f\"   Base Estimators: {best_results['Base_Estimators']}\")\n",
    "            print(f\"   Meta Regressor: {best_results['Meta_Regressor']}\")\n",
    "        \n",
    "        return self.best_model, self.best_model_name, self.best_score\n",
    "\n",
    "    def print_results_summary(self):\n",
    "        \"\"\"\n",
    "        Print a comprehensive summary of all results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Dataset Information\n",
    "        print(f\"\\n📊 DATASET INFORMATION:\")\n",
    "        print(f\"   Original Shape: {self.df_raw.shape}\")\n",
    "        print(f\"   Final Shape: {self.df.shape}\")\n",
    "        print(f\"   Features Dropped: {len(self.features_to_drop)}\")\n",
    "        print(f\"   Categorical Features: {len(self.categorical_cols)}\")\n",
    "        print(f\"   Numerical Features: {len(self.numerical_cols)}\")\n",
    "        \n",
    "        # Model Performance Summary\n",
    "        print(f\"\\n🤖 MODEL PERFORMANCE SUMMARY:\")\n",
    "        \n",
    "        # Base Models\n",
    "        if hasattr(self, 'results') and self.results:\n",
    "            print(f\"\\n   Base Models ({len(self.results)} models):\")\n",
    "            base_sorted = sorted(self.results.items(), \n",
    "                               key=lambda x: x[1]['Test_Custom_Score'], \n",
    "                               reverse=True)\n",
    "            for i, (name, results) in enumerate(base_sorted[:3], 1):\n",
    "                print(f\"      {i}. {name}: {results['Test_Custom_Score']:.4f}\")\n",
    "        \n",
    "        # Tuned Models\n",
    "        if hasattr(self, 'tuned_results') and self.tuned_results:\n",
    "            print(f\"\\n   Tuned Models ({len(self.tuned_results)} models):\")\n",
    "            tuned_sorted = sorted(self.tuned_results.items(), \n",
    "                                key=lambda x: x[1]['Test_Custom_Score'], \n",
    "                                reverse=True)\n",
    "            for i, (name, results) in enumerate(tuned_sorted, 1):\n",
    "                print(f\"      {i}. {name}: {results['Test_Custom_Score']:.4f}\")\n",
    "        \n",
    "        # Stacking Models\n",
    "        if hasattr(self, 'stacking_results') and self.stacking_results:\n",
    "            print(f\"\\n   Stacking Models ({len(self.stacking_results)} models):\")\n",
    "            stacking_sorted = sorted(self.stacking_results.items(), \n",
    "                                   key=lambda x: x[1]['Test_Custom_Score'], \n",
    "                                   reverse=True)\n",
    "            for i, (name, results) in enumerate(stacking_sorted, 1):\n",
    "                print(f\"      {i}. {name}: {results['Test_Custom_Score']:.4f}\")\n",
    "        \n",
    "        # Best Model Information\n",
    "        if hasattr(self, 'best_model_name'):\n",
    "            print(f\"\\n🏆 BEST OVERALL MODEL:\")\n",
    "            print(f\"   Model: {self.best_model_name}\")\n",
    "            print(f\"   Test Custom Score: {self.best_score:.4f}\")\n",
    "            print(f\"   Test RMSE: {self.best_results['Test_RMSE']:.4f}\")\n",
    "            print(f\"   Test R²: {self.best_results['Test_R2']:.4f}\")\n",
    "            print(f\"   Train Custom Score: {self.best_results['Train_Custom_Score']:.4f}\")\n",
    "            overfitting = self.best_results['Train_Custom_Score'] - self.best_results['Test_Custom_Score']\n",
    "            print(f\"   Overfitting Gap: {overfitting:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    def predict(self, X_new):\n",
    "        \"\"\"\n",
    "        Make predictions on new data using the complete pipeline\n",
    "        \"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"No model has been trained yet. Run the pipeline first.\")\n",
    "        \n",
    "        print(\"Making predictions on new data...\")\n",
    "        \n",
    "        # Step 1: Fix data types (same as training)\n",
    "        X_processed = self.fix_data_types(X_new.copy(), \"NEW DATA\")\n",
    "        \n",
    "        # Step 2: Apply imputation pipeline\n",
    "        X_processed = self.imputer.transform(X_processed)\n",
    "        \n",
    "        # Step 3: Apply feature engineering\n",
    "        from utils.feature_engineering import SolarFeatureEngineering\n",
    "        feature_engineer = SolarFeatureEngineering()\n",
    "        X_processed = feature_engineer.create_solar_features(X_processed)\n",
    "        \n",
    "        # Step 4: Drop features that were dropped during training\n",
    "        if self.features_to_drop:\n",
    "            available_features = [col for col in self.features_to_drop if col in X_processed.columns]\n",
    "            if available_features:\n",
    "                X_processed = X_processed.drop(columns=available_features)\n",
    "        \n",
    "        # Step 5: Select only the features used in training\n",
    "        X_processed = X_processed[self.feature_cols]\n",
    "        \n",
    "        # Step 6: Apply categorical encoding\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X_processed.columns:\n",
    "                # Handle unseen categories by using the most frequent category\n",
    "                try:\n",
    "                    X_processed[col] = self.label_encoders[col].transform(X_processed[col].astype(str))\n",
    "                except ValueError:\n",
    "                    # If unseen categories, replace with most frequent\n",
    "                    most_frequent_encoded = 0  # Assuming first category is most frequent\n",
    "                    X_processed[col] = X_processed[col].apply(\n",
    "                        lambda x: self.label_encoders[col].transform([str(x)])[0] \n",
    "                        if str(x) in self.label_encoders[col].classes_ \n",
    "                        else most_frequent_encoded\n",
    "                    )\n",
    "        \n",
    "        # Step 7: Apply preprocessing\n",
    "        X_scaled = self.preprocessor.transform(X_processed)\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        feature_names = self.numerical_cols + self.categorical_cols\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "        \n",
    "        # Step 8: Make predictions (on transformed scale)\n",
    "        predictions_transformed = self.best_model.predict(X_scaled)\n",
    "        \n",
    "        # Step 9: Transform predictions back to original scale\n",
    "        predictions_original = self.inverse_transform_predictions(predictions_transformed)\n",
    "        \n",
    "        print(f\"Predictions completed for {len(predictions_original)} samples\")\n",
    "        \n",
    "        return predictions_original\n",
    "\n",
    "    def save_best_model(self, filepath='model/best_solar_model_complete.pkl'):\n",
    "        \"\"\"\n",
    "        Save the best model and all preprocessing components\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.best_model,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'label_encoders': self.label_encoders,\n",
    "            'target_transformer': self.target_transformer,\n",
    "            'imputer': self.imputer,\n",
    "            'feature_names': self.feature_cols,\n",
    "            'categorical_cols': self.categorical_cols,\n",
    "            'numerical_cols': self.numerical_cols,\n",
    "            'best_model_name': self.best_model_name,\n",
    "            'best_score': self.best_score,\n",
    "            'features_to_drop': self.features_to_drop,\n",
    "            'best_results': self.best_results,\n",
    "            'target_col': self.target_col\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"✅ Best model with complete pipeline saved to {filepath}\")\n",
    "        print(f\"   Model: {self.best_model_name}\")\n",
    "        print(f\"   Score: {self.best_score:.4f}\")\n",
    "    \n",
    "    def load_model(self, filepath='model/best_solar_model_complete.pkl'):\n",
    "        \"\"\"\n",
    "        Load a saved model with complete pipeline\n",
    "        \"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_package = pickle.load(f)\n",
    "        \n",
    "        self.best_model = model_package['model']\n",
    "        self.preprocessor = model_package['preprocessor']\n",
    "        self.label_encoders = model_package['label_encoders']\n",
    "        self.target_transformer = model_package['target_transformer']\n",
    "        self.imputer = model_package['imputer']\n",
    "        self.feature_cols = model_package['feature_names']\n",
    "        self.categorical_cols = model_package['categorical_cols']\n",
    "        self.numerical_cols = model_package['numerical_cols']\n",
    "        self.best_model_name = model_package.get('best_model_name', 'Unknown')\n",
    "        self.best_score = model_package.get('best_score', 0)\n",
    "        self.features_to_drop = model_package.get('features_to_drop', [])\n",
    "        self.best_results = model_package.get('best_results', {})\n",
    "        self.target_col = model_package.get('target_col', 'efficiency')\n",
    "        \n",
    "        print(f\"✅ Model with complete pipeline loaded successfully\")\n",
    "        print(f\"   Model: {self.best_model_name}\")\n",
    "        print(f\"   Score: {self.best_score:.4f}\")\n",
    "        \n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete model selection pipeline including stacking\n",
    "        \"\"\"\n",
    "        print(\"🚀 STARTING COMPLETE SOLAR PANEL MODEL SELECTION PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and prepare data\n",
    "            print(\"\\n📊 STEP 1: Data Loading and Preparation\")\n",
    "            self.load_and_prepare_data()\n",
    "            \n",
    "            # Step 2: Create preprocessing pipeline\n",
    "            print(\"\\n🔧 STEP 2: Creating Preprocessing Pipeline\")\n",
    "            self.create_preprocessing_pipeline()\n",
    "            \n",
    "            # Step 3: Prepare train-test split\n",
    "            print(\"\\n✂️ STEP 3: Train-Test Split\")\n",
    "            self.prepare_train_test_split()\n",
    "            \n",
    "            # Step 4: Define and evaluate base models\n",
    "            print(\"\\n🤖 STEP 4: Base Model Evaluation\")\n",
    "            self.define_models()\n",
    "            self.evaluate_base_models()\n",
    "            \n",
    "            # Step 5: Hyperparameter tuning\n",
    "            print(\"\\n⚙️ STEP 5: Hyperparameter Tuning\")\n",
    "            self.hyperparameter_tuning(top_n=5)\n",
    "            \n",
    "            # Step 6: Create stacking models\n",
    "            print(\"\\n🏗️ STEP 6: Stacking Model Creation\")\n",
    "            self.create_stacking_models(n_best=2)\n",
    "            \n",
    "            # Step 7: Select best model\n",
    "            print(\"\\n🎯 STEP 7: Best Model Selection\")\n",
    "            self.select_best_model()\n",
    "            \n",
    "            # Step 8: Print comprehensive results\n",
    "            print(\"\\n📊 STEP 8: Results Summary\")\n",
    "            self.print_results_summary()\n",
    "            \n",
    "            # Step 9: Save best model\n",
    "            print(\"\\n💾 STEP 9: Saving Best Model\")\n",
    "            self.save_best_model()\n",
    "            \n",
    "            print(\"\\n🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"🏆 Best Model: {self.best_model_name}\")\n",
    "            print(f\"📈 Final Test Score: {self.best_score:.4f}\")\n",
    "            \n",
    "            return self.best_model, self.best_model_name, self.best_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ PIPELINE FAILED: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Usage example and main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🌞 SOLAR PANEL PERFORMANCE MODEL SELECTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define features to drop based on your analysis\n",
    "    features_to_drop = [\n",
    "        'id', 'temperature', 'humidity', 'maintenance_count', 'voltage',\n",
    "        'module_temperature', 'pressure', 'string_id', 'error_code',\n",
    "        'installation_type', 'power_output', 'temp_difference',\n",
    "        'temp_coefficient_effect', 'expected_irradiance_clean', 'age_category',\n",
    "        'environmental_stress', 'effective_module_temp', 'power_output_log',\n",
    "        'temp_difference_robust', 'performance_deviation', 'efficiency_ratio',\n",
    "        'mean', 'std', 'min', 'max', 'power_output_string_mean',\n",
    "        'power_output_string_std', 'power_output_string_min',\n",
    "        'power_output_string_max', 'power_vs_string_mean', 'error_indicator',\n",
    "        'consecutive_errors', 'anomaly_score', 'operating_regime',\n",
    "        'regime_expected_power', 'regime_performance_deviation'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Initialize the model selector\n",
    "        print(\"🔧 Initializing model selector...\")\n",
    "        selector = SolarPanelModelSelector(\n",
    "            data_path='dataset/train.csv', \n",
    "            features_to_drop=features_to_drop,\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Run the complete pipeline\n",
    "        print(\"🚀 Starting complete pipeline...\")\n",
    "        best_model, best_model_name, best_score = selector.run_complete_pipeline()\n",
    "        \n",
    "        print(f\"\\n🎊 FINAL RESULTS:\")\n",
    "        print(f\"   Best Model: {best_model_name}\")\n",
    "        print(f\"   Best Score: {best_score:.4f}\")\n",
    "        \n",
    "        # Example of making predictions on new data\n",
    "        print(f\"\\n📝 USAGE EXAMPLES:\")\n",
    "        print(f\"   # Load new data and make predictions:\")\n",
    "        print(f\"   # new_data = pd.read_csv('new_data.csv')\")\n",
    "        print(f\"   # predictions = selector.predict(new_data)\")\n",
    "        print(f\"   \")\n",
    "        print(f\"   # Load saved model in new session:\")\n",
    "        print(f\"   # new_selector = SolarPanelModelSelector()\")\n",
    "        print(f\"   # new_selector.load_model('model/best_solar_model_complete.pkl')\")\n",
    "        print(f\"   # predictions = new_selector.predict(new_data)\")\n",
    "        \n",
    "        # Demonstrate model loading\n",
    "        print(f\"\\n🔄 Testing model save/load functionality...\")\n",
    "        \n",
    "        # Save current model\n",
    "        selector.save_best_model('model/test_model.pkl')\n",
    "        \n",
    "        # Create new instance and load model\n",
    "        new_selector = SolarPanelModelSelector()\n",
    "        new_selector.load_model('model/test_model.pkl')\n",
    "        \n",
    "        print(f\"✅ Model save/load test successful!\")\n",
    "        print(f\"   Loaded model: {new_selector.best_model_name}\")\n",
    "        print(f\"   Loaded score: {new_selector.best_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n🏁 EXECUTION COMPLETED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
